Ollama adapter to integrate [Ollama](https://ollama.com/) self-hosted LLMs into your prompt workflows.

You need to install the Ollama CLI and have a model running locally. You can find instructions on how to do that [here](https://ollama.com/docs/getting-started/installation).

_Tip: When running from browser, you need to [allow CORS for the Ollama server](https://chatgpt.com/share/682db7e4-ef94-800e-91b2-087955828628)._
