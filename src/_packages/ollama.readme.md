Ollama adapter to integrate [Ollama](https://ollama.com/) self-hosted LLMs into your prompt workflows.

You need to install the Ollama CLI and have a model running locally. You can find instructions on how to do that [here](https://ollama.com/docs/getting-started/installation).
